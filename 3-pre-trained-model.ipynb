{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, DistilBertTokenizerFast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned datasets\n",
    "train_df = pd.read_csv('E:\\\\Vocational\\\\Lighthouse Labs\\\\Flex Course\\\\Projects\\\\P05_Large Language Models\\\\llm_project\\\\data\\\\cleaned_train.csv.gz', compression='gzip')\n",
    "test_df = pd.read_csv('E:\\\\Vocational\\\\Lighthouse Labs\\\\Flex Course\\\\Projects\\\\P05_Large Language Models\\\\llm_project\\\\data\\\\cleaned_test.csv.gz', compression='gzip')\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16476\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment analysis pipeline with DistilBERT\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Function to get predictions from the pipeline\n",
    "def get_predictions(text_list, classifier, max_length=512):\n",
    "    # Use the classifier pipeline directly, which handles tokenization internally\n",
    "    preds = classifier(text_list, padding=True, truncation=True, max_length=max_length)\n",
    "    # Extract the labels from the predictions\n",
    "    pred_labels = [1 if pred['label'] == 'POSITIVE' else 0 for pred in preds]\n",
    "    return pred_labels\n",
    "\n",
    "# Get predictions for train and test datasets\n",
    "train_preds = get_predictions(train_df['clean_text'].tolist(), classifier)\n",
    "test_preds = get_predictions(test_df['clean_text'].tolist(), classifier)\n",
    "\n",
    "# Verify predictions\n",
    "print(train_preds[:10])\n",
    "print(test_preds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Evaluation:\n",
      "Accuracy: 0.7956954706071314\n",
      "Precision: 0.9187840290381125\n",
      "Recall: 0.6494547787042976\n",
      "F1 Score: 0.7609921082299888\n",
      "Confusion Matrix:\n",
      "[[11716   716]\n",
      " [ 4372  8100]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82     12432\n",
      "           1       0.92      0.65      0.76     12472\n",
      "\n",
      "    accuracy                           0.80     24904\n",
      "   macro avg       0.82      0.80      0.79     24904\n",
      "weighted avg       0.82      0.80      0.79     24904\n",
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.7987984355469537\n",
      "Precision: 0.920999095840868\n",
      "Recall: 0.655064308681672\n",
      "F1 Score: 0.7655956407365652\n",
      "Confusion Matrix:\n",
      "[[11662   699]\n",
      " [ 4291  8149]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82     12361\n",
      "           1       0.92      0.66      0.77     12440\n",
      "\n",
      "    accuracy                           0.80     24801\n",
      "   macro avg       0.83      0.80      0.79     24801\n",
      "weighted avg       0.83      0.80      0.79     24801\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7987984355469537,\n",
       " 0.920999095840868,\n",
       " 0.655064308681672,\n",
       " 0.7655956407365652,\n",
       " array([[11662,   699],\n",
       "        [ 4291,  8149]], dtype=int64),\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.73      0.94      0.82     12361\\n           1       0.92      0.66      0.77     12440\\n\\n    accuracy                           0.80     24801\\n   macro avg       0.83      0.80      0.79     24801\\nweighted avg       0.83      0.80      0.79     24801\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    print('Classification Report:')\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm, report\n",
    "\n",
    "# Evaluate train predictions\n",
    "print(\"Train Set Evaluation:\")\n",
    "evaluate_model(y_train, train_preds)\n",
    "\n",
    "# Evaluate test predictions\n",
    "print(\"Test Set Evaluation:\")\n",
    "evaluate_model(y_test, test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
